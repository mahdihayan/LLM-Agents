{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJ2DxjZfZyd+O1klzPpT0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/How-to-apply-popular-LLMs/blob/main/Chatbot_Gradio_OpenAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBYpfTG74MjE",
        "outputId": "2957b6bc-5c1c-4661-da1c-817c706edd4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.6/318.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.3.0 requires websockets<15.0dev,>=13.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#First install the necesary libraries\n",
        "!pip install -q openai==1.1.1\n",
        "!pip install -q gradio==4.40.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if you need a API Key from OpenAI\n",
        "#https://platform.openai.com/account/api-keys\n",
        "import openai\n",
        "import random\n",
        "import gradio as gr\n",
        "\n",
        "from getpass import getpass\n",
        "openai.api_key=getpass(\"OpenAI API Key:\")\n",
        "model = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EbG-i8n4Zim",
        "outputId": "50c3115f-474d-4baf-f71c-6d414a0dc874"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the system part of the prompt\n",
        "#Read and understand it.\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You work collecting orders in a delivery IceCream shop called\n",
        "I'm freezed.\n",
        "\n",
        "First welcome the customer, in a very friedly way, then collects the order.\n",
        "\n",
        "Your instuctions are:\n",
        "-Collect the entire order, only from options in our menu, toppings included.\n",
        "-Summarize it\n",
        "-check for a final time if everithing is ok or the customer wants to add anything else.\n",
        "-collect the payment, be sure to include topings and the size of the ice cream.\n",
        "-Make sure to clarify all options, extras and sizes to uniquely\n",
        "identify the item from the menu.\n",
        "-Your answer should be short in a very friendly style.\n",
        "\n",
        "Our Menu:\n",
        "The IceCream menu includes only the flavors:\n",
        "-Vainilla.\n",
        "-Chocolate.\n",
        "-Lemon.\n",
        "-Strawberry.\n",
        "-Coffe.\n",
        "\n",
        "The IceCreams are available in two sizes:\n",
        "-Big: 3$\n",
        "-Medium: 2$\n",
        "\n",
        "Toppings:\n",
        "-Caramel\n",
        "-White chocolate\n",
        "-melted peanut butter\n",
        "Each topping cost 0.5$\n",
        "\n",
        "\"\"\"} ]"
      ],
      "metadata": {
        "id": "TPSVXpeh5Lpk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will receive the different messages in the conversation,\n",
        "#and call OpenAI passing the full conversartion.\n",
        "def continue_conversation(messages, temperature=0):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "jMeHJAtl5UGc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The function that Gradio will use.\n",
        "def gradio_chat(message, history):\n",
        "    #Add the instructions to the prompt.\n",
        "    history_chat = context\n",
        "\n",
        "    #Add the history that Gradio send to us.\n",
        "    for user, assistant in history:\n",
        "        history_chat.append({\"role\":\"user\", \"content\":user})\n",
        "        history_chat.append({\"role\":\"assistant\", \"content\":assistant})\n",
        "\n",
        "    #Add the las user message.\n",
        "    history_chat.append({\"role\":\"user\", \"content\":message})\n",
        "\n",
        "    #Call OpenAI and return the response.\n",
        "    return continue_conversation(history_chat, 0)"
      ],
      "metadata": {
        "id": "KFyN2C8z5cY_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Customized gradio textbox.\n",
        "InputText = gr.Textbox(label=\"order\", info=\"Your Order here.\", scale= 6)"
      ],
      "metadata": {
        "id": "eGbilFEa8C9h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(gradio_chat,\n",
        "                 textbox=InputText,\n",
        "                 retry_btn=None,\n",
        "                 undo_btn=None,\n",
        "                 title=\"I'm freezed\",\n",
        "                submit_btn=\"Order\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "Lg46tRyY51lB",
        "outputId": "f658b640-87a7-4d5c-fc4b-42ac7ac838ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.40.0, however version 4.44.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2808d219b9d1f46972.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2808d219b9d1f46972.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UG3VMGRd97lI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
