{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/LLM-Agents/blob/main/10_bleu_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcSolREOl8Gl"
      },
      "source": [
        "# Evaluating translations with BLEU\n",
        "Models: nllb-200-distilled-600M\n",
        "\n",
        "Colab environment: CPU.\n",
        "\n",
        "Keys:\n",
        "* Bleu Evaluation.\n",
        "* Translation Pipeline.\n",
        "* Google Translator API.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "lz_2jJZpnSKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVAkT7wBZjhZ"
      },
      "source": [
        "In this notebook, we will use the BLEU metric to compare the quality of two different approaches for performing translations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C72gH1FT6ANK"
      },
      "outputs": [],
      "source": [
        "# Install the correct versions of packages\n",
        "!pip install -q numpy==1.26.4\n",
        "!pip install -q transformers==4.42.1 evaluate==0.4.2 googletrans==3.1.0a0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eGbFnGU75_Tv"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BDp4f1CQp3hS"
      },
      "outputs": [],
      "source": [
        "#Sentences to Translate.\n",
        "sentences = [\n",
        "    \"در فصل‌های قبلی، بیشتر با مدل‌های OpenAI کار کردید و با مدل‌های متن‌باز Hugging Face، استفاده از تعبیه‌ها، پایگاه‌های داده برداری و ایجنت‌ها آشنا شدید.\",\n",
        "    \"این فصل‌ها بسیار کاربردی بودند و سعی کردم مفاهیمی را به‌تدریج معرفی کنم که به شما کمک کرده‌اند دانش خود را گسترش دهید و پروژه‌هایی با استفاده از فناوری‌های مدرن مدل‌های زبانی بزرگ ایجاد کنید.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KwvAUtyGbNQ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reference translations (for BLEU score calculation)\n",
        "reference_translations = [\n",
        "    [\"In the previous chapters, you mostly worked with OpenAI models and got a practical introduction to Hugging Face's open-source models, the use of embeddings, vector databases, and agents.\"],\n",
        "    [\"These were very practical chapters where I tried to gradually introduce concepts that helped you expand your knowledge and start building projects using the modern technology stack of large language models.\"]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ3Wm2Z9gCTf"
      },
      "source": [
        "##Create Translations with NLLB.\n",
        "We will perform the first translation using the NLLB model, a small model specialized in performing translations, which we will retrieve from Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDlw6xChiHf4"
      },
      "source": [
        "When creating the pipeline, we pass the source language and the target language of the translation to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SEQT4ae9R2M5"
      },
      "outputs": [],
      "source": [
        "model_id = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator_nllb = pipeline('translation', model=model, tokenizer=tokenizer,\n",
        "                           src_lang=\"fas_Arab\", tgt_lang=\"eng_Latn\")\n",
        "\n",
        "# ترجمه با مدل NLLB\n",
        "translations_nllb = []\n",
        "for text in sentences:\n",
        "    print(\"To translate (NLLB):\", text)\n",
        "    translation = translator_nllb(text)\n",
        "    translations_nllb.append(translation[0]['translation_text'])\n",
        "    print(translation[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr8ETAMVs7t-",
        "outputId": "dca4e85d-65d0-40c0-d02e-54a9e7487775"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To translate (NLLB): در فصل‌های قبلی، بیشتر با مدل‌های OpenAI کار کردید و با مدل‌های متن‌باز Hugging Face، استفاده از تعبیه‌ها، پایگاه‌های داده برداری و ایجنت‌ها آشنا شدید.\n",
            "In previous seasons, you've mostly worked with OpenAI models and have been very familiar with open-source Hugging Face models, the use of packages, databases and agents.\n",
            "To translate (NLLB): این فصل‌ها بسیار کاربردی بودند و سعی کردم مفاهیمی را به‌تدریج معرفی کنم که به شما کمک کرده‌اند دانش خود را گسترش دهید و پروژه‌هایی با استفاده از فناوری‌های مدرن مدل‌های زبانی بزرگ ایجاد کنید.\n",
            "These chapters were very useful and I tried to introduce concepts gradually that have helped you expand your knowledge and create projects using modern technologies, large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmtO4nZemvRK"
      },
      "source": [
        "##Create Translations with Google Traslator.\n",
        "\n",
        "As a second source for translations, we will use the Google Translator API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vGS0JZ0GWMe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c811484c-ac21-4803-84c9-58b82261be22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To translate (Google): در فصل‌های قبلی، بیشتر با مدل‌های OpenAI کار کردید و با مدل‌های متن‌باز Hugging Face، استفاده از تعبیه‌ها، پایگاه‌های داده برداری و ایجنت‌ها آشنا شدید.\n",
            "In the previous chapters, you were mostly working with Openai models and familiar with the Hugging Face opening models, the use of embeddeds, databases and agents.\n",
            "To translate (Google): این فصل‌ها بسیار کاربردی بودند و سعی کردم مفاهیمی را به‌تدریج معرفی کنم که به شما کمک کرده‌اند دانش خود را گسترش دهید و پروژه‌هایی با استفاده از فناوری‌های مدرن مدل‌های زبانی بزرگ ایجاد کنید.\n",
            "These chapters were very practical and I tried to gradually introduce concepts that have helped you expand your knowledge and create projects using modern technologies of large language models.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "translator_google = Translator()\n",
        "translations_google = []\n",
        "for text in sentences:\n",
        "    print(\"To translate (Google):\", text)\n",
        "    translation = translator_google.translate(text, dest=\"en\")\n",
        "    translations_google.append(translation.text)\n",
        "    print(translation.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0M4Z5cUqC5z"
      },
      "source": [
        "In this list, we have the translations created by Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXuT5sl8ptCo"
      },
      "source": [
        "## Evaluate translations with BLEU\n",
        "\n",
        "We will use the BLEU implementation from the Evaluate library by Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BLEU scores using sacrebleu\n",
        "# Calculate BLEU for NLLB model\n",
        "bleu_nllb = sacrebleu.corpus_bleu (translations_nllb, [reference_translations])\n",
        "print(\"BLEU Score for NLLB Model: \", bleu_nllb.score)\n",
        "\n",
        "# Calculate BLEU for Google Translate\n",
        "bleu_google = sacrebleu.corpus_bleu(translations_google, [reference_translations])\n",
        "print(\"BLEU Score for Google Translate: \", bleu_google.score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "lFl8IiN5u4dq",
        "outputId": "624decab-aed4-4823-a6c7-8eddf8b3a86d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BLEU: `refs` should be a sequence of sequence of strings.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-356e3f19578c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate BLEU scores using sacrebleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Calculate BLEU for NLLB model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbleu_nllb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msacrebleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_bleu\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtranslations_nllb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreference_translations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BLEU Score for NLLB Model: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_nllb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sacrebleu/compat.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(hypotheses, references, smooth_method, smooth_value, force, lowercase, tokenize, use_effective_order)\u001b[0m\n\u001b[1;32m     35\u001b[0m         effective_order=use_effective_order)\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sacrebleu/metrics/base.py\u001b[0m in \u001b[0;36mcorpus_score\u001b[0;34m(self, hypotheses, references, n_bootstrap)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScore\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \"\"\"\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_score_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# Collect corpus stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sacrebleu/metrics/base.py\u001b[0m in \u001b[0;36m_check_corpus_score_args\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix}: {err_msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BLEU: `refs` should be a sequence of sequence of strings."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgXkAq3r4mz"
      },
      "source": [
        "It appears that the translation performed by the Google API is significantly better than the one performed by the NLLB model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}