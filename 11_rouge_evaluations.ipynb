{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/LLM-Agents/blob/main/11_rouge_evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff896d43",
      "metadata": {
        "id": "ff896d43"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/4-Evaluating%20LLMs/4_1_rouge_evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L-e2T5XtdgOQ",
      "metadata": {
        "id": "L-e2T5XtdgOQ"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>4.1-BLEU,  ROUGE and N-Grams. </h2>\n",
        "    <h3>Evaluating summaries with ROUGE </h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "_______\n",
        "Models: t5-base-cnn / t5-base\n",
        "\n",
        "Colab Environment: CPU\n",
        "\n",
        "Keys:\n",
        "* Summary Evaluation.\n",
        "* N-Grams.\n",
        "* Rouge.\n",
        "\n",
        "Related article: [ROUGE Metrics: Evaluating Summaries in Large Language Models](https://pub.towardsai.net/rouge-metrics-evaluating-summaries-in-large-language-models-d200ee7ca0e6)\n",
        "_______\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2feaa00c",
      "metadata": {
        "id": "2feaa00c",
        "papermill": {
          "duration": 0.012459,
          "end_time": "2023-08-14T22:02:21.395806",
          "exception": false,
          "start_time": "2023-08-14T22:02:21.383347",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# How to Evaluate Large Language Models for Summarization Using ROUGE.\n",
        "The way we evaluate large language models is quite different from evaluating machine learning models, where metrics like Accuracy, F1 Score, or Recall were commonly used.\n",
        "\n",
        "Metrics for generated language are distinct. Depending on the specific application, different metrics are chosen to assess the model's performance.\n",
        "\n",
        "In this notebook, we will explore the usage of the ROUGE metric to measure the quality of summaries generated by a language model.\n",
        "\n",
        "## What is ROUGE?\n",
        "ROUGE isn't just a single metric; it's a set of metrics that measure the overlap and similarity between the generated summary and a reference summary that serves as a benchmark.\n",
        "\n",
        "It returns fourth individual metrics. The metrics provided are:\n",
        "\n",
        "* ROUGE-1: Measures the overlap of unigrams, or single words.\n",
        "* ROUGE-2: Measures the overlap of bigrams, or pairs of words.\n",
        "* ROUGE-L: Measures the longest common subsequence, rewarding longer shared sequences between the generated and reference summaries.\n",
        "* ROUGE-LSUM: Calculated as the length of the LCS divided by the sum of the lengths of the generated summary and the reference summary.\n",
        "\n",
        "## What are we going to do?\n",
        "We are going to use two T5 models, one of them being the t5-Base model and the other a t5-base fine-tuned  specifically designed for creating summaries.\n",
        "\n",
        "First, we will use a dataset and generate summaries using both models. By comparing the two generated summaries, we can observe whether the fine-tuning has been effective in producing different results. In other words, here we will only determine that the two models exhibit significant differences in summary generation, but we won't know which one might perform better.\n",
        "\n",
        "To determine which model generates better summaries, we will utilize a well-known dataset called 'cnn_dailymail,' which is available in the 'datasets' library.\n",
        "\n",
        "This dataset contains reference summaries that can be used for comparison. We will assess the summaries generated by the two models against these reference summaries.\n",
        "\n",
        "The model that obtains a higher ROUGE score will be considered the one that produces better summaries.\n",
        "\n",
        "## The models.\n",
        "t5-Base Finnetunned: https://huggingface.co/flax-community/t5-base-cnn-dm\n",
        "\n",
        "t5-Base: https://huggingface.co/t5-base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T79v8QczFmsk",
      "metadata": {
        "id": "T79v8QczFmsk"
      },
      "outputs": [],
      "source": [
        "!pip install -q evaluate==0.4.2\n",
        "!pip install -q transformers==4.42.4\n",
        "!pip install -q rouge_score==0.1.2\n",
        "!pip install kaggle\n",
        "#!pip install -q datasets==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U414LeQpHFnh",
      "metadata": {
        "id": "U414LeQpHFnh"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import evaluate\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6466b7f5",
      "metadata": {
        "id": "6466b7f5",
        "papermill": {
          "duration": 0.013868,
          "end_time": "2023-08-14T22:02:21.4225",
          "exception": false,
          "start_time": "2023-08-14T22:02:21.408632",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bfad0c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:21.450466Z",
          "iopub.status.busy": "2023-08-14T22:02:21.449273Z",
          "iopub.status.idle": "2023-08-14T22:02:25.012019Z",
          "shell.execute_reply": "2023-08-14T22:02:25.011006Z"
        },
        "id": "c1bfad0c",
        "papermill": {
          "duration": 3.579718,
          "end_time": "2023-08-14T22:02:25.015059",
          "exception": false,
          "start_time": "2023-08-14T22:02:21.435341",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Import generic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9463e9f0",
      "metadata": {
        "id": "9463e9f0",
        "papermill": {
          "duration": 0.012577,
          "end_time": "2023-08-14T22:02:25.040898",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.028321",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "The dataset is available on Kaggle and comprises a collection of technological news articles compiled by MIT. The article text is located in the 'Article Body' column.\n",
        "\n",
        "https://www.kaggle.com/datasets/deepanshudalal09/mit-ai-news-published-till-2023\n",
        "\n",
        "## Importing Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zm2Bv97P5VFc",
      "metadata": {
        "id": "Zm2Bv97P5VFc"
      },
      "source": [
        "Yo only need acces to the articles.csv file from the Dataaset, you can download and load it directly, if you prefer to use the API Kaggle you can use the code Below. To use the Kaggle API you will need to hace your kaggle.json file with your keys in the directory /content/drive/MyDrive/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_3a0v2GghP1",
      "metadata": {
        "id": "-_3a0v2GghP1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r75XOrBuhY6m",
      "metadata": {
        "id": "r75XOrBuhY6m"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d deepanshudalal09/mit-ai-news-published-till-2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVhE7Sr_h8cf",
      "metadata": {
        "id": "zVhE7Sr_h8cf"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "file_path = '/content/mit-ai-news-published-till-2023.zip'\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "   zip_ref.extractall('/content/drive/MyDrive/kaggle')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wXm0WI235KRV",
      "metadata": {
        "id": "wXm0WI235KRV"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed7edd6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:25.068957Z",
          "iopub.status.busy": "2023-08-14T22:02:25.06828Z",
          "iopub.status.idle": "2023-08-14T22:02:25.270255Z",
          "shell.execute_reply": "2023-08-14T22:02:25.269362Z"
        },
        "id": "fed7edd6",
        "papermill": {
          "duration": 0.219225,
          "end_time": "2023-08-14T22:02:25.273176",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.053951",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "news = pd.read_csv('/content/drive/MyDrive/kaggle/articles.csv')\n",
        "DOCUMENT=\"Article Body\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13d6679",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:25.305575Z",
          "iopub.status.busy": "2023-08-14T22:02:25.304951Z",
          "iopub.status.idle": "2023-08-14T22:02:25.310307Z",
          "shell.execute_reply": "2023-08-14T22:02:25.309257Z"
        },
        "id": "e13d6679",
        "papermill": {
          "duration": 0.022404,
          "end_time": "2023-08-14T22:02:25.313066",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.290662",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Because it is just a course we select a small portion of News.\n",
        "MAX_NEWS = 3\n",
        "subset_news = news.head(MAX_NEWS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ebc0be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:25.346059Z",
          "iopub.status.busy": "2023-08-14T22:02:25.344835Z",
          "iopub.status.idle": "2023-08-14T22:02:25.368428Z",
          "shell.execute_reply": "2023-08-14T22:02:25.367384Z"
        },
        "id": "d0ebc0be",
        "papermill": {
          "duration": 0.04048,
          "end_time": "2023-08-14T22:02:25.370798",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.330318",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "subset_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7e9329",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:25.400413Z",
          "iopub.status.busy": "2023-08-14T22:02:25.399937Z",
          "iopub.status.idle": "2023-08-14T22:02:25.405688Z",
          "shell.execute_reply": "2023-08-14T22:02:25.404595Z"
        },
        "id": "ad7e9329",
        "papermill": {
          "duration": 0.024357,
          "end_time": "2023-08-14T22:02:25.40825",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.383893",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "articles = subset_news[DOCUMENT].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359993aa",
      "metadata": {
        "id": "359993aa",
        "papermill": {
          "duration": 0.013052,
          "end_time": "2023-08-14T22:02:25.434756",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.421704",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Load the Models and create the summaries\n",
        "\n",
        "Both models are available on Hugging Face, so we will work with the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906947a6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:25.463399Z",
          "iopub.status.busy": "2023-08-14T22:02:25.462666Z",
          "iopub.status.idle": "2023-08-14T22:02:27.592118Z",
          "shell.execute_reply": "2023-08-14T22:02:27.59115Z"
        },
        "id": "906947a6",
        "papermill": {
          "duration": 2.146867,
          "end_time": "2023-08-14T22:02:27.594898",
          "exception": false,
          "start_time": "2023-08-14T22:02:25.448031",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_name_base = \"t5-base\"\n",
        "model_name_finetuned = \"flax-community/t5-base-cnn-dm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544b7159",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:27.623394Z",
          "iopub.status.busy": "2023-08-14T22:02:27.622565Z",
          "iopub.status.idle": "2023-08-14T22:02:27.627878Z",
          "shell.execute_reply": "2023-08-14T22:02:27.627035Z"
        },
        "id": "544b7159",
        "papermill": {
          "duration": 0.022031,
          "end_time": "2023-08-14T22:02:27.630092",
          "exception": false,
          "start_time": "2023-08-14T22:02:27.608061",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#This function returns the tokenizer and the Model.\n",
        "def get_model(model_id):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "    return tokenizer, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbaa67f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:27.658002Z",
          "iopub.status.busy": "2023-08-14T22:02:27.657508Z",
          "iopub.status.idle": "2023-08-14T22:02:48.180447Z",
          "shell.execute_reply": "2023-08-14T22:02:48.178839Z"
        },
        "id": "fbbaa67f",
        "papermill": {
          "duration": 20.540525,
          "end_time": "2023-08-14T22:02:48.183564",
          "exception": false,
          "start_time": "2023-08-14T22:02:27.643039",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer_base, model_base = get_model(model_name_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7934f671",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:02:48.215342Z",
          "iopub.status.busy": "2023-08-14T22:02:48.214347Z",
          "iopub.status.idle": "2023-08-14T22:03:35.738349Z",
          "shell.execute_reply": "2023-08-14T22:03:35.736901Z"
        },
        "id": "7934f671",
        "papermill": {
          "duration": 47.542959,
          "end_time": "2023-08-14T22:03:35.74126",
          "exception": false,
          "start_time": "2023-08-14T22:02:48.198301",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer_finetuned, model_finetuned = get_model(model_name_finetuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a423d166",
      "metadata": {
        "id": "a423d166",
        "papermill": {
          "duration": 0.015182,
          "end_time": "2023-08-14T22:03:35.772039",
          "exception": false,
          "start_time": "2023-08-14T22:03:35.756857",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "With both models downloaded and ready, we create a function that will perform the summaries.\n",
        "\n",
        "The function takes fourth parameters:\n",
        "\n",
        "* the list of texts to summarize.\n",
        "* the tokenizer.\n",
        "* the model.\n",
        "* the maximum length for the generated summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rs2D656lEzCW",
      "metadata": {
        "id": "Rs2D656lEzCW"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc6d899",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:03:35.805553Z",
          "iopub.status.busy": "2023-08-14T22:03:35.804721Z",
          "iopub.status.idle": "2023-08-14T22:03:35.813463Z",
          "shell.execute_reply": "2023-08-14T22:03:35.812706Z"
        },
        "id": "7fc6d899",
        "papermill": {
          "duration": 0.028015,
          "end_time": "2023-08-14T22:03:35.815725",
          "exception": false,
          "start_time": "2023-08-14T22:03:35.78771",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_summaries(texts_list, tokenizer, model, max_l=125):\n",
        "\n",
        "    # We are going to add a prefix to each article to be summarized\n",
        "    # so that the model knows what it should do\n",
        "    prefix = \"Summarize this news: \"\n",
        "    summaries_list = [] #Will contain all summaries\n",
        "\n",
        "    texts_list = [prefix + text for text in texts_list]\n",
        "\n",
        "    for text in texts_list:\n",
        "\n",
        "        summary=\"\"\n",
        "\n",
        "        #calculate the encodings\n",
        "        input_encodings = tokenizer(text,\n",
        "                                    max_length=1024,\n",
        "                                    return_tensors='pt',\n",
        "                                    padding=True,\n",
        "                                    truncation=True)\n",
        "\n",
        "        # Generate summaries\n",
        "        start = time.time()\n",
        "        output = model.generate(\n",
        "            input_ids=input_encodings.input_ids,\n",
        "            attention_mask=input_encodings.attention_mask,\n",
        "            max_length=max_l,  # Set the maximum length of the generated summary\n",
        "            num_beams=2,     # Set the number of beams for beam search\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        #Decode to get the text\n",
        "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "        end = time.time()\n",
        "        #Add the summary to summaries list\n",
        "        elapsed_time = end - start\n",
        "        print(f\"Time taken: {elapsed_time:.3f} seconds\")\n",
        "        summaries_list += summary\n",
        "    return summaries_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffdb4fa",
      "metadata": {
        "id": "4ffdb4fa",
        "papermill": {
          "duration": 0.015207,
          "end_time": "2023-08-14T22:03:35.84634",
          "exception": false,
          "start_time": "2023-08-14T22:03:35.831133",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "To create the summaries, we call the 'create_summaries' function, passing both the news articles and the corresponding tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd00e31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:03:35.879443Z",
          "iopub.status.busy": "2023-08-14T22:03:35.878646Z",
          "iopub.status.idle": "2023-08-14T22:04:05.971552Z",
          "shell.execute_reply": "2023-08-14T22:04:05.970547Z"
        },
        "id": "ddd00e31",
        "papermill": {
          "duration": 30.112965,
          "end_time": "2023-08-14T22:04:05.974752",
          "exception": false,
          "start_time": "2023-08-14T22:03:35.861787",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Creating the summaries for both models.\n",
        "summaries_base = create_summaries(articles,\n",
        "                                  tokenizer_base,\n",
        "                                  model_base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2002eec6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:04:06.009738Z",
          "iopub.status.busy": "2023-08-14T22:04:06.008788Z",
          "iopub.status.idle": "2023-08-14T22:04:37.085514Z",
          "shell.execute_reply": "2023-08-14T22:04:37.084489Z"
        },
        "id": "2002eec6",
        "papermill": {
          "duration": 31.095807,
          "end_time": "2023-08-14T22:04:37.088051",
          "exception": false,
          "start_time": "2023-08-14T22:04:05.992244",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries_finetuned = create_summaries(articles,\n",
        "                                      tokenizer_finetuned,\n",
        "                                      model_finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df297d1b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:04:37.120847Z",
          "iopub.status.busy": "2023-08-14T22:04:37.120424Z",
          "iopub.status.idle": "2023-08-14T22:04:37.127523Z",
          "shell.execute_reply": "2023-08-14T22:04:37.126412Z"
        },
        "id": "df297d1b",
        "papermill": {
          "duration": 0.026332,
          "end_time": "2023-08-14T22:04:37.129872",
          "exception": false,
          "start_time": "2023-08-14T22:04:37.10354",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887bcc1a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T22:04:37.163043Z",
          "iopub.status.busy": "2023-08-14T22:04:37.16222Z",
          "iopub.status.idle": "2023-08-14T22:04:37.169083Z",
          "shell.execute_reply": "2023-08-14T22:04:37.168144Z"
        },
        "id": "887bcc1a",
        "papermill": {
          "duration": 0.026038,
          "end_time": "2023-08-14T22:04:37.171289",
          "exception": false,
          "start_time": "2023-08-14T22:04:37.145251",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries_finetuned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5feaed8e",
      "metadata": {
        "id": "5feaed8e",
        "papermill": {
          "duration": 0.015366,
          "end_time": "2023-08-14T22:04:37.20224",
          "exception": false,
          "start_time": "2023-08-14T22:04:37.186874",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "At first glance, it's evident that the summaries are different.\n",
        "\n",
        "However, it's challenging to determine which one is better.\n",
        "\n",
        "It's even difficult to discern whether they are significantly distinct or if there are just subtle differences between them.\n",
        "\n",
        "This is what we are going to verify now using ROUGE. When comparing the summaries of one model with those of the other, we don't get an idea of which one is better, but rather an idea of how much the summaries have changed with the fine-tuning applied to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d77e02c",
      "metadata": {
        "id": "6d77e02c",
        "papermill": {
          "duration": 0.015293,
          "end_time": "2023-08-14T22:04:37.23328",
          "exception": false,
          "start_time": "2023-08-14T22:04:37.217987",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# ROUGE\n",
        "Let's load the ROUEGE evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffc7d23e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T15:44:38.982027Z",
          "iopub.status.busy": "2023-08-14T15:44:38.980575Z",
          "iopub.status.idle": "2023-08-14T15:44:39.388636Z",
          "shell.execute_reply": "2023-08-14T15:44:39.387254Z",
          "shell.execute_reply.started": "2023-08-14T15:44:38.981986Z"
        },
        "id": "ffc7d23e",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#With the function load of the library evaluate\n",
        "#we create a rouge_score object\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e413b0",
      "metadata": {
        "id": "80e413b0",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Calculating ROUGE is as simple as calling the *compute* function of the *rouge_score* object we created earlier. This function takes the texts to compare as arguments and a third value *use_stemmer*, which indicates whether it should use *stemmer* or full words for the comparison.\n",
        "\n",
        "A *stemmer* is the base of the word. Transform differents forms of a word in a same base.\n",
        "\n",
        "Some samples of steammer are:\n",
        "* Jumping -> Jump.\n",
        "* Running -> Run.\n",
        "* Cats -> Cat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42c5f2f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:04:20.076601Z",
          "iopub.status.busy": "2023-08-14T16:04:20.076189Z",
          "iopub.status.idle": "2023-08-14T16:04:20.083634Z",
          "shell.execute_reply": "2023-08-14T16:04:20.082378Z",
          "shell.execute_reply.started": "2023-08-14T16:04:20.076571Z"
        },
        "id": "d42c5f2f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_rouge_score(generated, reference):\n",
        "\n",
        "    #We need to add '\\n' to each line before send it to ROUGE\n",
        "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
        "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
        "\n",
        "    return rouge_score.compute(\n",
        "        predictions=generated_with_newlines,\n",
        "        references=reference_with_newlines,\n",
        "        use_stemmer=True,\n",
        "\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eab715f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:04:20.824031Z",
          "iopub.status.busy": "2023-08-14T16:04:20.823653Z",
          "iopub.status.idle": "2023-08-14T16:04:21.027311Z",
          "shell.execute_reply": "2023-08-14T16:04:21.026243Z",
          "shell.execute_reply.started": "2023-08-14T16:04:20.824001Z"
        },
        "id": "6eab715f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "compute_rouge_score(summaries_base, summaries_finetuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a26c505",
      "metadata": {
        "id": "8a26c505",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "We can see that there is a difference between the two models when performing summarization.\n",
        "\n",
        "For example, in ROUGE-1, the similarity is 47%, while in ROUGE-2, it's a 32%. This indicates that the results are different, with some similarities but differents enough.\n",
        "\n",
        "However, we still don't know which model is better since we have compared them to each other and not to a reference text. But at the very least, we know that the fine-tuning process applied to the second model has significantly altered its results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4d432c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-07T17:21:28.296636Z",
          "iopub.status.busy": "2023-08-07T17:21:28.296173Z",
          "iopub.status.idle": "2023-08-07T17:21:28.301906Z",
          "shell.execute_reply": "2023-08-07T17:21:28.300702Z",
          "shell.execute_reply.started": "2023-08-07T17:21:28.296601Z"
        },
        "id": "bb4d432c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "# Comparing to a Dataset with real summaries.\n",
        "We are going to load the Dataset cnn_dailymail. This is a well-known dataset available in the **Datasets** library, and it suits our purpose perfectly.\n",
        "\n",
        "Apart from the news, it also contains pre-existing summaries.\n",
        "\n",
        "We will compare the summaries generated by the two models we are using with those from the dataset to determine which model creates summaries that are closer to the reference ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97babcd2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:26:09.837146Z",
          "iopub.status.busy": "2023-08-14T16:26:09.836684Z",
          "iopub.status.idle": "2023-08-14T16:29:24.075679Z",
          "shell.execute_reply": "2023-08-14T16:29:24.074315Z",
          "shell.execute_reply.started": "2023-08-14T16:26:09.837112Z"
        },
        "id": "97babcd2",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "cnn_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_eGIHx-lBlVC",
      "metadata": {
        "id": "_eGIHx-lBlVC"
      },
      "outputs": [],
      "source": [
        "#Get just a few news to test\n",
        "sample_cnn = cnn_dataset[\"test\"].select(range(MAX_NEWS))\n",
        "\n",
        "sample_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be652a76",
      "metadata": {
        "id": "be652a76",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "We retrieve the maximum length of the summaries to give the models the option to generate summaries of the same length, if they choose to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8546de4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:29:24.079892Z",
          "iopub.status.busy": "2023-08-14T16:29:24.078748Z",
          "iopub.status.idle": "2023-08-14T16:29:24.0898Z",
          "shell.execute_reply": "2023-08-14T16:29:24.085476Z",
          "shell.execute_reply.started": "2023-08-14T16:29:24.079854Z"
        },
        "id": "e8546de4",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "max_length = max(len(item['highlights']) for item in sample_cnn)\n",
        "max_length = max_length + 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9389d76f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:29:24.093133Z",
          "iopub.status.busy": "2023-08-14T16:29:24.091136Z",
          "iopub.status.idle": "2023-08-14T16:29:46.747981Z",
          "shell.execute_reply": "2023-08-14T16:29:46.746942Z",
          "shell.execute_reply.started": "2023-08-14T16:29:24.093079Z"
        },
        "id": "9389d76f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries_t5_base = create_summaries(sample_cnn[\"article\"],\n",
        "                                      tokenizer_base,\n",
        "                                      model_base,\n",
        "                                      max_l=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ab5985",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:29:46.751951Z",
          "iopub.status.busy": "2023-08-14T16:29:46.750888Z",
          "iopub.status.idle": "2023-08-14T16:30:05.204078Z",
          "shell.execute_reply": "2023-08-14T16:30:05.203025Z",
          "shell.execute_reply.started": "2023-08-14T16:29:46.7519Z"
        },
        "id": "67ab5985",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries_t5_finetuned = create_summaries(sample_cnn[\"article\"],\n",
        "                                      tokenizer_finetuned,\n",
        "                                      model_finetuned,\n",
        "                                      max_l=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b80c0c1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:35:57.777001Z",
          "iopub.status.busy": "2023-08-14T16:35:57.776036Z",
          "iopub.status.idle": "2023-08-14T16:35:57.784049Z",
          "shell.execute_reply": "2023-08-14T16:35:57.782555Z",
          "shell.execute_reply.started": "2023-08-14T16:35:57.776958Z"
        },
        "id": "7b80c0c1",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Get the real summaries from the cnn_dataset\n",
        "real_summaries = sample_cnn['highlights']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8dc3a10",
      "metadata": {
        "id": "a8dc3a10",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Let's take a look at the generated summaries alongside the reference summaries provided by the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ea4e8c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:36:00.561736Z",
          "iopub.status.busy": "2023-08-14T16:36:00.561112Z",
          "iopub.status.idle": "2023-08-14T16:36:00.577942Z",
          "shell.execute_reply": "2023-08-14T16:36:00.576678Z",
          "shell.execute_reply.started": "2023-08-14T16:36:00.561681Z"
        },
        "id": "d6ea4e8c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "summaries = pd.DataFrame.from_dict(\n",
        "        {\n",
        "            \"base\": summaries_t5_base,\n",
        "            \"finetuned\": summaries_t5_finetuned,\n",
        "            \"reference\": real_summaries,\n",
        "        }\n",
        "    )\n",
        "summaries.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73e7277",
      "metadata": {
        "id": "c73e7277",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Now we can calculate the ROUGE scores for the two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F5zYqTY4vsQ1",
      "metadata": {
        "id": "F5zYqTY4vsQ1"
      },
      "outputs": [],
      "source": [
        "summaries_t5_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vSwf6biYvz6P",
      "metadata": {
        "id": "vSwf6biYvz6P"
      },
      "outputs": [],
      "source": [
        "real_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0bda484",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:37:53.508688Z",
          "iopub.status.busy": "2023-08-14T16:37:53.506732Z",
          "iopub.status.idle": "2023-08-14T16:37:53.827647Z",
          "shell.execute_reply": "2023-08-14T16:37:53.826103Z",
          "shell.execute_reply.started": "2023-08-14T16:37:53.508613Z"
        },
        "id": "c0bda484",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "compute_rouge_score(summaries_t5_base, real_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4045072",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-14T16:37:55.512869Z",
          "iopub.status.busy": "2023-08-14T16:37:55.512163Z",
          "iopub.status.idle": "2023-08-14T16:37:55.822842Z",
          "shell.execute_reply": "2023-08-14T16:37:55.821446Z",
          "shell.execute_reply.started": "2023-08-14T16:37:55.512794Z"
        },
        "id": "a4045072",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "compute_rouge_score(summaries_t5_finetuned, real_summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91b3f50",
      "metadata": {
        "id": "c91b3f50",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "With these results, I would say that the fine-tuned model performs slightly better than the T5-Base model. It consistently achieves higher ROUGE scores in all metrics except for LSUM, where the difference is minimal.\n",
        "\n",
        "Additionally, the ROUGE metrics are quite interpretable.\n",
        "\n",
        "LSUM indicates the percentage of the longest common subsequence, regardless of word order, in relation to the total length of the text.\n",
        "\n",
        "This can be a good indicator of overall similarity between texts. However, both models have very similar LSUM scores, and the fine-tuned model has better scores in other ROUGE metrics.\n",
        "\n",
        "Personally, I would lean towards the fine-tuned model, although the difference may not be very significant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vX2Z-ALxUsbt",
      "metadata": {
        "id": "vX2Z-ALxUsbt"
      },
      "source": [
        "### Comparing entities with ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_Gg7nk3wGqS",
      "metadata": {
        "id": "B_Gg7nk3wGqS"
      },
      "outputs": [],
      "source": [
        "entities=['Paris, Londres, Barcelona, Reus']\n",
        "entities_ref=['Reus, Paris, Londres, Barcelona']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RKHNNE86wZwz",
      "metadata": {
        "id": "RKHNNE86wZwz"
      },
      "outputs": [],
      "source": [
        "compute_rouge_score(entities, entities_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VA25Pali08XD",
      "metadata": {
        "id": "VA25Pali08XD"
      },
      "outputs": [],
      "source": [
        "entities_ref=['Paris, Londres, Barcelona, Reus']\n",
        "compute_rouge_score(entities, entities_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nuHe-5aWLPeu",
      "metadata": {
        "id": "nuHe-5aWLPeu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 150.75,
      "end_time": "2023-08-14T22:04:41.050595",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-08-14T22:02:10.300595",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}